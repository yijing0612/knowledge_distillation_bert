# ğŸ”¬ Knowledge Distillation & LoRA Fine-tuning for BERT

This project demonstrates two efficient approaches to compress or fine-tune BERT models for text classification tasks:

- âœ… **Knowledge Distillation**: Train a compact "student" model to mimic a large "teacher" model (e.g., `bert-large-uncased` âœ `bert-base-uncased`).
- âœ… **LoRA Fine-tuning**: Use Low-Rank Adaptation (LoRA) to fine-tune BERT in a parameter-efficient way by injecting trainable adapters.

**Dataset Used**: [AG News](https://huggingface.co/datasets/ag_news)

---

## ğŸ“ Project Structure

knowledge_distillation_bert/
â”œâ”€â”€ data/ # Tokenized datasets (generated)
â”œâ”€â”€ distillation/ # Distillation training scripts
â”‚ â””â”€â”€ train_distill.py
â”‚ â””â”€â”€ evaluate.py
â”œâ”€â”€ lora_finetune/ # LoRA fine-tuning scripts
â”‚ â””â”€â”€ train_lora.py
â”œâ”€â”€ models/ # Student model definition
â”‚ â””â”€â”€ student_model.py
â”œâ”€â”€ checkpoints/ # Saved model checkpoints
â”œâ”€â”€ tokenize_dataset.py # Dataset preprocessing script
â”œâ”€â”€ evaluate.py # Unified evaluation script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---
